MixtralAWQForCausalLM(
  (model): MixtralForCausalLM(
    (model): MixtralModel(
      (embed_tokens): Embedding(32000, 4096)
      (layers): ModuleList(
        (0-31): 32 x MixtralDecoderLayer(
          (self_attn): MixtralSdpaAttention(
            (q_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128)
            (k_proj): WQLinear_GEMM(in_features=4096, out_features=1024, bias=False, w_bit=4, group_size=128)
            (v_proj): WQLinear_GEMM(in_features=4096, out_features=1024, bias=False, w_bit=4, group_size=128)
            (o_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=False, w_bit=4, group_size=128)
            (rotary_emb): MixtralRotaryEmbedding()
          )
          (block_sparse_moe): MixtralSparseMoeBlock(
            (gate): Linear(in_features=4096, out_features=8, bias=False)
            (experts): ModuleList(
              (0-7): 8 x MixtralBlockSparseTop2MLP(
                (w1): WQLinear_GEMM(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)
                (w2): WQLinear_GEMM(in_features=14336, out_features=4096, bias=False, w_bit=4, group_size=128)
                (w3): WQLinear_GEMM(in_features=4096, out_features=14336, bias=False, w_bit=4, group_size=128)
                (act_fn): SiLU()
              )
            )
          )
          (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)
          (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)
        )
      )
      (norm): MixtralRMSNorm((4096,), eps=1e-05)
    )
    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  )
)
