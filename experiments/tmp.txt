HERE
HERE
DONE
MixtralModel(
  (embed_tokens): Embedding(32000, 4096)
  (layers): ModuleList(
    (0-31): 32 x MixtralDecoderLayer(
      (self_attn): MixtralSdpaAttention(
        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
        (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): MixtralRotaryEmbedding()
      )
      (block_sparse_moe): MixtralSparseMoeBlock(
        (gate): Linear4bit(in_features=4096, out_features=8, bias=False)
        (experts): ModuleList(
          (0-7): 8 x MixtralBlockSparseTop2MLP(
            (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)
            (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)
            (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)
            (act_fn): SiLU()
          )
        )
      )
      (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)
      (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)
    )
  )
  (norm): MixtralRMSNorm((4096,), eps=1e-05)
)
DONE
